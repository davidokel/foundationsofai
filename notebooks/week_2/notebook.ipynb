{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing and geometry in a feature space\n",
    "\n",
    "This coursework is designed to practically explore the content given in the lectures in week 3.\n",
    "\n",
    "## Hints\n",
    "\n",
    "- This one is a lot more challenging than last week so don't panic, break everything down into small steps\n",
    "\n",
    "- For the further work questions it may be helpful to write your code using functions rather than scripts (or even classes if they are familiar to you)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#DO NOT CHANGE THIS CODE\n",
    "X_train=np.loadtxt('./data/X_train_linear.txt')\n",
    "X_test=np.loadtxt('./data/X_test_linear.txt')\n",
    "y_train=np.loadtxt('./data/y_train_linear.txt')\n",
    "y_test=np.loadtxt('./data/y_test_linear.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Geometry in a kernel-defined feature space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Compute the linear kernel of the linear training data without centering the data and calculate the norm of the mean of a sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In original space: 0.6018046905945809\n",
      "In kernel space: [[0.60180469]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "K=linear_kernel(X_train)\n",
    "\n",
    "print(f\"In original space: {np.linalg.norm(X_train.mean(axis=0))}\")\n",
    "\n",
    "m=X_train.shape[0]\n",
    "j=np.ones((m,1))\n",
    "mean_of_a_sample=np.sqrt(j.T@K@j)/m\n",
    "\n",
    "print(f\"In kernel space: {mean_of_a_sample}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected the mean of the sample can be calculated in both the original ...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Calculate the expected squared distance to the mean of a sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected squared distance in kernel feature space: [[19.23293439]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Expected squared distance in kernel feature space: {np.trace(K)/m-j.T@K@j/m**2}\")\n",
    "\n",
    "#(np.linalg.norm(X_train - X_train.mean(axis=0),axis=1)**2).mean()\n",
    "#distance = X_train-sample_mean\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) By subtracting the appropriate matrix, implement the centering of this kernel matrix and verify that the norm of the mean of a sample is zero"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of a sample in centred kernel: [[-1.94527949e-16]]\n"
     ]
    }
   ],
   "source": [
    "m=X_train.shape[0]\n",
    "j=np.ones((m,1))\n",
    "K_hat=K-1/m*(j@j.T@K+K@j@j.T)+1/m**2*(j.T@K@j)*j@j.T\n",
    "print(f\"mean of a sample in centred kernel: {(j.T@K_hat@j)/m**2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d) Verify that the expected squared distance to the mean of a sample with the centred kernel is the same as the uncentred kernel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected squared distance in kernel feature space: [[19.23293439]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected squared distance in kernel feature space: {np.trace(K_hat)/m-j.T@K_hat@j/m**2}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e) By centering the original data and computing the linear kernel representation of the centered data show that the kernel representations are equivalent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "data": {
      "text/plain": "1.4210854715202004e-14"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(linear_kernel(X_train-X_train.mean(axis=0))-K_hat))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Simple Classification Algorithm\n",
    "In this part we will build the simple classifier described in the lectures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#DO NOT CHANGE THIS CODE\n",
    "X_train=np.loadtxt('./data/X_train_poly.txt')\n",
    "X_test=np.loadtxt('./data/X_test_poly.txt')\n",
    "y_train=np.loadtxt('./data/y_train_poly.txt')\n",
    "y_test=np.loadtxt('./data/y_test_poly.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Write a function which calculates the mean of the kernel function of a sample with the positive and negative training data (i.e. the terms that aren't b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "def get_m(y):\n",
    "    m_pos=y[y>0].shape[0]\n",
    "    m_neg=y[y<0].shape[0]\n",
    "    return m_pos, m_neg\n",
    "\n",
    "def kernel_function(K,y):\n",
    "    m_pos,m_neg=get_m(y)\n",
    "    positive_mean=K[:,y>0].sum(axis=1)/m_pos\n",
    "    negative_mean=K[:,y<0].sum(axis=1)/m_neg\n",
    "    return positive_mean-negative_mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 6.25609520e-01, -2.32656312e+00,  7.60642583e-01, -5.80356108e-01,\n       -1.48849059e+00, -1.86896475e+00, -2.08447008e+00, -1.71472451e-01,\n       -2.72415073e+00,  9.51467627e-01, -2.98943905e-01, -1.22342750e+00,\n       -2.01697207e-01, -3.28055737e-01, -5.80513275e-01,  2.20288617e+00,\n       -2.37536560e-01,  2.85227617e-02, -4.48560416e-01, -3.78632030e-02,\n        1.75133072e+00,  2.22555739e+00,  8.68513078e-01,  2.02140156e+00,\n        1.98771729e+00,  1.94415532e+00, -1.12283622e+00, -1.35696469e+00,\n       -5.49945993e-01, -2.07905923e+00, -1.80405842e+00, -6.55863262e-02,\n        1.97259319e+00,  8.32757865e-02, -1.64279493e+00, -4.97748307e-01,\n       -1.43308834e+00, -2.67113416e+00,  1.03157183e+00, -1.31540116e+00,\n        1.46732812e+00, -7.76615082e-02,  6.27688534e-01, -2.93553951e+00,\n       -1.29553608e+00,  3.44035866e+00,  2.31744131e+00, -1.74318130e-01,\n        2.77729299e+00, -2.49241431e-01,  1.51421362e+00, -6.97346386e-01,\n       -9.38259052e-01, -9.67239562e-01, -1.74320245e+00,  1.16070083e-01,\n        1.10359373e+00,  1.88486221e+00,  1.12029933e+00, -9.25999606e-01,\n       -1.76836830e+00, -1.03848987e-01,  5.75741725e-01, -1.86063687e+00,\n       -1.47034759e+00, -3.97692435e+00, -2.21978990e+00, -4.97785540e-01,\n       -1.37995671e+00, -6.03414111e-02, -8.94611054e-02, -1.97907704e+00,\n       -1.06500694e+00,  1.38601364e+00, -5.60740856e-01, -3.08739431e-01,\n        1.00890895e+00,  3.21531619e+00, -1.50809657e+00, -6.57583936e-01,\n        2.86162290e-01, -1.08109989e+00,  2.58196661e-01, -1.98320595e+00,\n       -8.21118955e-01,  1.36655221e+00,  5.34489813e-01,  9.76382642e-01,\n       -1.18585647e+00, -1.26500609e+00,  4.17630315e-01, -1.47136849e+00,\n        7.64849327e-01,  1.80604680e+00,  9.06583152e-01,  2.48140262e-01,\n       -1.21770583e+00,  1.04831157e+00,  8.80142464e-01, -1.30406293e+00,\n        2.74206452e+00,  2.34847231e+00,  2.35448683e-01,  1.22874590e+00,\n       -5.55563678e-01,  2.64263199e+00,  9.33067136e-01, -3.97196481e-02,\n        1.63794108e+00,  5.62659407e-01, -3.58416868e-01,  2.40566004e+00,\n        3.41616793e-01,  7.99192025e-03,  1.58949947e+00,  3.29161705e+00,\n       -8.41487081e-02,  1.58280050e+00,  6.88562704e-01, -1.15584145e+00,\n       -1.55128813e+00,  8.67142385e-02,  1.84353994e+00,  1.58159522e+00,\n       -1.70316702e+00,  2.49387559e+00,  1.30811068e+00, -4.87750126e-01,\n        1.26808123e+00, -3.92815241e-01,  9.75743949e-01, -1.94592475e+00,\n        5.45170834e-01,  2.56025139e-01,  2.87624014e-01, -4.54351692e-01,\n        5.55711454e-01,  2.13071674e+00,  1.99893154e+00, -2.68246100e-01,\n       -5.51001065e-01,  1.30176433e-02, -1.66583073e+00,  3.03742885e-01,\n       -1.74988980e+00,  4.91565389e+00,  7.05446318e-01, -2.08719276e+00,\n       -1.87611508e+00,  2.76300477e+00, -3.20173157e-01,  2.04149039e+00,\n        2.08093527e+00, -6.05220418e-01,  2.22295713e+00,  2.59537031e+00,\n        1.40170474e+00,  1.40994065e+00,  1.11703907e+00, -5.51909035e-01,\n       -1.22937525e+00,  1.48905036e+00, -3.78355587e-01,  4.73475558e-01,\n        1.44960395e+00,  2.29334162e+00, -4.05692416e-01, -3.63226816e-01,\n       -5.89455020e-01,  3.79676085e+00,  6.22761520e-01, -3.21961231e-01,\n        6.38252694e-02,  1.49239111e+00, -3.23463538e+00, -1.29903832e+00,\n        1.65588550e+00, -5.98493797e-01, -1.98883223e+00, -5.39161181e-01,\n       -7.06289867e-01,  9.71780853e-01, -5.18747337e-02,  1.89165899e+00,\n        1.55678528e+00,  1.68278398e+00, -2.87839710e-01,  1.29544775e+00,\n       -5.22709884e+00,  1.40346951e+00,  2.83616050e+00, -2.06557945e+00,\n        6.41874965e-01,  4.00056896e-01, -2.72230335e+00, -1.97314936e+00,\n       -6.56957007e-01,  2.02221886e+00,  2.83493518e+00,  5.96615133e-01,\n        1.73993209e-01,  2.77206864e+00,  3.08443412e+00, -9.44078410e-01,\n        1.06056974e+00, -1.75288427e-01, -1.11686481e+00,  2.17226113e+00,\n        1.11364342e-01, -1.05632759e-02, -9.37790446e-01, -1.38134710e+00,\n        6.24905395e-01,  1.56444921e-01,  3.28224670e+00, -1.70117973e-01,\n        2.10624905e+00, -3.14914024e+00,  3.80553707e-01,  2.94517501e+00,\n       -8.87763721e-01, -4.61926087e-01,  4.07596328e-01,  2.86084085e-01,\n        2.24155453e+00, -3.99776109e+00,  2.80355228e+00, -2.19348869e-03,\n        5.87145172e-01,  5.77016586e-02,  1.06291513e+00, -9.02645435e-01,\n       -3.71456438e-01,  7.45555498e-01,  8.82689249e-01, -1.05403362e+00,\n       -6.80762753e-02,  1.14900227e+00, -2.41885707e+00, -1.14613516e+00,\n        2.32635182e+00, -7.92372166e-01,  1.62182809e-01,  1.67315359e+00,\n        1.77748457e+00, -4.82963620e-01,  2.27030271e+00, -3.13544281e-02,\n       -2.02136744e-01,  3.73718704e+00])"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=linear_kernel(X_train)\n",
    "get_m(y_train)\n",
    "kernel_function(K,y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Write a function which calculates the b term in the simple classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "def calculate_b(K,y):\n",
    "    m_pos,m_neg=get_m(y)\n",
    "    return K[y>0,y>0].sum()/m_pos**2-K[y<0,y<0].sum()/m_neg**2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Write the decision function of the classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "def simple_classifier(K_test,K_train,y_train):\n",
    "    return np.sign(kernel_function(K_test,y_train)-calculate_b(K_train,y_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Calculate the accuracy of this simple classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accruacy: 0.82\n",
      "Test accruacy: 0.808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "K_train=linear_kernel(X_train)\n",
    "K_test=linear_kernel(X_test,X_train)\n",
    "\n",
    "y_pred_train=simple_classifier(K_train,K_train,y_train)\n",
    "y_pred_test=simple_classifier(K_test,K_train, y_train)\n",
    "\n",
    "print(f\"Train accruacy: {accuracy_score(y_train,y_pred_train)}\")\n",
    "print(f\"Test accruacy: {accuracy_score(y_test,y_pred_test)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Fisher Discriminant Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Write a function which calculates the matrices B, C, and D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "def calculate_C(y):\n",
    "    m_pos,m_neg=get_m(y)\n",
    "    m=m_pos+m_neg\n",
    "    positive_entry=2*m_neg/(m*m_pos)\n",
    "    negative_entry=2*m_pos/(m*m_neg)\n",
    "    C=np.zeros((m,m))\n",
    "    C[y>0,y>0]=positive_entry\n",
    "    C[y<0,y<0]=negative_entry\n",
    "    return C\n",
    "\n",
    "def calculate_D(y):\n",
    "    m_pos,m_neg=get_m(y)\n",
    "    m=m_pos+m_neg\n",
    "    diag=np.zeros(m)\n",
    "    diag[y>0]=2*m_neg/m\n",
    "    diag[y<0]=2*m_pos/m\n",
    "    D=np.diag(diag)\n",
    "    return D\n",
    "\n",
    "def calculate_B(y):\n",
    "    D=calculate_D(y)\n",
    "    C=calculate_C(y)\n",
    "    return D-C"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Write a function which calculates the dual variables for the FDA classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "def get_alpha(K,y,lam=0.001):\n",
    "    B=calculate_B(y)\n",
    "    return np.linalg.inv(B@K+lam*np.eye(K.shape[0]))@y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Write a function which calculates t and therefore the offset term b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "def get_t(y):\n",
    "    m_pos,m_neg=get_m(y)\n",
    "    t=np.ones_like(y)\n",
    "    t[y>0]=1/m_pos\n",
    "    t[y<0]=1/m_neg\n",
    "    return t\n",
    "\n",
    "def get_b(alpha,K,t):\n",
    "    return 0.5*alpha@K@t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d) Write the decision function of the classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "def fda(K_test,K_train,y_train,lam=1000):\n",
    "    alpha=get_alpha(K_train,y_train,lam=lam)\n",
    "    t=get_t(y_train)\n",
    "    b=get_b(alpha,K_train,t)\n",
    "    return np.sign(K_test@alpha-b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Further Investigation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) For one or both classifiers Demonstrate how the performance of the classifier changes if one class is overrepresented in the training data as compared to the test data (i.e. there are imbalanced classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accruacy: 1.0\n",
      "Test accruacy: 0.608\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "\n",
    "K_train=polynomial_kernel(X_train,degree=6)\n",
    "K_test=polynomial_kernel(X_test,X_train,degree=6)\n",
    "\n",
    "y_pred_train=fda(K_train,K_train,y_train, lam=0.0000000001)\n",
    "y_pred_test=fda(K_test,K_train, y_train,lam=0.00000001)\n",
    "\n",
    "print(f\"Train accruacy: {accuracy_score(y_train,y_pred_train)}\")\n",
    "print(f\"Test accruacy: {accuracy_score(y_test,y_pred_test)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Demonstrate how regularisation can be used to improve the generalisation of the FDA classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
