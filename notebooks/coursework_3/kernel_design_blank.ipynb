{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and Data\n",
    "\n",
    "We will be working with the 20 newsgroups dataset.\n",
    "\n",
    "I have included the minimal sufficient imports to complete the coursework. You are welcome to import other modules.\n",
    "\n",
    "**some functions will produce sparse matrices for which operations may not work as you expect. Make sure to check the dimensionality of your outputs. You should be able to convert to normal arrays with .toarray() if you have any problems**\n",
    "\n",
    "**This coursework follows the Kernels for text section in the slides**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import KernelPCA\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'), subset='train', categories=['sci.electronics','rec.autos','rec.sport.hockey'])\n",
    "train_documents = dataset.data\n",
    "train_targets=dataset.target\n",
    "train_target_names=dataset.target_names\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'), subset='test', categories=['sci.electronics','rec.autos','rec.sport.hockey'])\n",
    "test_documents = dataset.data\n",
    "test_targets=dataset.target\n",
    "test_target_names=dataset.target_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Using CountVectorizer or otherwise, convert the list of documents to"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Use CountVectorizer to get the counts of each term (word) in each of the documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Write a function which returns the term frequency of each of the terms across all of the training documents. Use np.asarray() and np.squeeze() to ensure this produces a 1d output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Using numpy.histogram followed by plt.hist, plot a histogram of the log of the term frequencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e) Write a function which returns the term inverse document frequency"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### f) Write a function which returns the tfidf weighted kernel for two sets of documents (i.e. train and test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### g) Use an SVC classifier with the outputs of CountVectorizer (ie the term counts for each document) to classify the documents according to the train_targets and test_targets and present the performance of this classifier in a suitable way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### h) Construct the tfidf kernel for the train (n_train x n_train) and test (n_test x n_train) data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### i) use the SVC classifier again to classify the documents. Compare the performance to the previous case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### j) Extension: Use (Kernel/Linear) PCA with varying dimensions of your tfidf kernel and repeat the classification or explore the eigenvalues of the kernel PCA and comment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
